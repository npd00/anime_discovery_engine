{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/npd00/anime_discovery_engine/blob/main/anime_discovery_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1y1VTT8jxom"
      },
      "source": [
        "# Personal Anime Discovery Engine\n",
        "\n",
        "**Objective:** Ingest Trakt.tv history, analyze viewing habits, and generate personalized anime recommendations using Google Gemini."
      ],
      "id": "v1y1VTT8jxom"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuGl9l3Pjxon"
      },
      "outputs": [],
      "source": [
        "# Step 1: Configuration & Path Discovery\n",
        "import os\n",
        "import glob\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Determine Environment (Colab vs Local)\n",
        "PROJECT_DIR = '.'\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    from google.colab import userdata\n",
        "    drive.mount('/content/drive')\n",
        "    PROJECT_DIR = '/content/drive/MyDrive/Trakt_Project'\n",
        "    print(\"‚úÖ Running in Google Colab\")\n",
        "except ImportError:\n",
        "    PROJECT_DIR = os.getcwd()\n",
        "    print(f\"‚úÖ Running Locally at: {PROJECT_DIR}\")\n",
        "\n",
        "DB_PATH = os.path.join(PROJECT_DIR, 'trakt_data.duckdb')\n",
        "MAX_CONTEXT_ITEMS = 50\n",
        "\n",
        "# Ensure project directory exists\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "print(f\"Project Directory: {PROJECT_DIR}\")\n",
        "\n",
        "# --- Interactive Configuration ---\n",
        "ALL_SERVICES = ['NETFLIX', 'CRUNCHYROLL', 'HIDIVE', 'HULU', 'DISNEY+', 'PRIME VIDEO', 'HBO MAX']\n",
        "service_checkboxes = [widgets.Checkbox(value=False, description=s) for s in ALL_SERVICES]\n",
        "for i in [0, 1, 2]:\n",
        "    if i < len(service_checkboxes): service_checkboxes[i].value = True\n",
        "services_ui = widgets.GridBox(service_checkboxes, layout=widgets.Layout(grid_template_columns=\"repeat(3, 200px)\"))\n",
        "\n",
        "ALL_GENRES = ['Action', 'Adventure', 'Cyberpunk', 'Isekai', 'Psychological Thriller', 'Mecha', 'Slice of Life', 'Fantasy', 'Sci-Fi', 'Horror', 'Romance', 'Sports', 'Mystery']\n",
        "genre_selector = widgets.SelectMultiple(options=ALL_GENRES, value=['Cyberpunk', 'Isekai'], rows=6, description='Genres:')\n",
        "\n",
        "mood_selector = widgets.Dropdown(options=['Any', 'Chill', 'Dark', 'Hype', 'Emotional', 'Complex'], value='Any', description='Mood:')\n",
        "discovery_mode = widgets.Dropdown(options=['Balanced', 'Safe Bets', 'Hidden Gems', 'Experimental'], value='Balanced', description='Mode:')\n",
        "time_commitment = widgets.Dropdown(options=['Any', 'Movie', 'Short (<13)', 'Medium (24)', 'Long'], value='Any', description='Length:')\n",
        "\n",
        "display(services_ui)\n",
        "display(widgets.HBox([genre_selector, widgets.VBox([mood_selector, discovery_mode, time_commitment])]))"
      ],
      "id": "CuGl9l3Pjxon"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-uqBoXojxoo"
      },
      "outputs": [],
      "source": [
        "# Step 1.5: DEBUG PATHS\n",
        "print(\"--- PATH DEBUGGER ---\")\n",
        "print(f\"Base: {PROJECT_DIR}\")\n",
        "search_pattern = os.path.join(PROJECT_DIR, '**', 'watched-history-*.json')\n",
        "found_files = glob.glob(search_pattern, recursive=True)\n",
        "print(f\"Files found via glob: {len(found_files)}\")"
      ],
      "id": "c-uqBoXojxoo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frS6LlR0jxoo"
      },
      "outputs": [],
      "source": [
        "# Step 2: Imports & Setup\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Configure Gemini\n",
        "try:\n",
        "    try: GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    except: GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')\n",
        "\n",
        "    if GEMINI_API_KEY:\n",
        "        genai.configure(api_key=GEMINI_API_KEY)\n",
        "        print(\"Gemini API Configured Successfully.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Gemini API Key not found. AI features disabled.\")\n",
        "except Exception as e:\n",
        "    print(f\"API Config Error: {e}\")"
      ],
      "id": "frS6LlR0jxoo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5ITM11ljxoo"
      },
      "outputs": [],
      "source": [
        "# Step 3: Database Initialization (With Cache Tables)\n",
        "\n",
        "def init_db():\n",
        "    con = duckdb.connect(DB_PATH)\n",
        "    con.execute(\"CREATE TABLE IF NOT EXISTS USER_WATCH_HISTORY (TRAKT_ID VARCHAR, TITLE VARCHAR, YEAR INTEGER, USER_RATING INTEGER, MEDIA_TYPE VARCHAR, GENRES VARCHAR, EPISODES_WATCHED INTEGER, TOTAL_EPISODES INTEGER, FIRST_WATCHED_AT TIMESTAMP, LAST_WATCHED_AT TIMESTAMP, WATCH_DURATION_DAYS INTEGER, BINGE_INDICATOR BOOLEAN, STUDIO VARCHAR, MAL_SCORE DOUBLE, VALID_FROM TIMESTAMP, VALID_TO TIMESTAMP, IS_CURRENT BOOLEAN, INGESTION_TIMESTAMP TIMESTAMP)\")\n",
        "    con.execute(\"CREATE TABLE IF NOT EXISTS PLAN_TO_WATCH (ID BIGINT PRIMARY KEY, TITLE VARCHAR, RECOMMENDED_BY_AI_AT TIMESTAMP, STREAMING_SERVICE VARCHAR, NOTES VARCHAR, PRIORITY VARCHAR, CONFIDENCE_SCORE DOUBLE)\")\n",
        "    con.execute(\"CREATE TABLE IF NOT EXISTS JIKAN_CACHE (TITLE VARCHAR PRIMARY KEY, MAL_ID INTEGER, DATA_JSON VARCHAR, FETCHED_AT TIMESTAMP)\")\n",
        "    con.execute(\"CREATE TABLE IF NOT EXISTS GENRE_CACHE (TITLE VARCHAR PRIMARY KEY, GENRES_JSON VARCHAR)\")\n",
        "    con.execute(\"CREATE SEQUENCE IF NOT EXISTS seq_plan_id START 1\")\n",
        "    con.close()\n",
        "    print(\"Database initialized.\")\n",
        "init_db()"
      ],
      "id": "Z5ITM11ljxoo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rr4XX5sjxoo"
      },
      "outputs": [],
      "source": [
        "# Step 4: ETL Functions\n",
        "\n",
        "def clean_title(title):\n",
        "    if not title: return \"UNKNOWN\"\n",
        "    return re.sub(r'[^A-Z0-9 \\-!:&]', '', str(title).upper()).strip()\n",
        "\n",
        "def enrich_with_jikan(items_list, con=None):\n",
        "    if not items_list: return {}\n",
        "    print(f\"\\nüîé Enriching {len(items_list)} titles via Jikan...\")\n",
        "    should_close = False\n",
        "    if not con: con = duckdb.connect(DB_PATH); should_close = True\n",
        "\n",
        "    # 1. Bulk Load Cache\n",
        "    try: cached_rows = con.execute(\"SELECT TITLE, DATA_JSON FROM JIKAN_CACHE\").fetchall()\n",
        "    except: cached_rows = []\n",
        "    cache_map = {r[0]: json.loads(r[1]) for r in cached_rows}\n",
        "\n",
        "    # 2. Identify Delta\n",
        "    results = {}\n",
        "    titles_to_process = []\n",
        "    for item in items_list:\n",
        "        t = item['TITLE']\n",
        "        if t in cache_map:\n",
        "            results[t] = cache_map[t] # Use cached raw data momentarily\n",
        "        else:\n",
        "            titles_to_process.append(item)\n",
        "\n",
        "    print(f\"   - Cached: {len(results)} | New to Fetch: {len(titles_to_process)}\")\n",
        "\n",
        "    # 3. Fetch Missing\n",
        "    for item in titles_to_process:\n",
        "        title = item['TITLE']; year_trakt = item['YEAR']\n",
        "        data = None\n",
        "        try:\n",
        "            url = f\"https://api.jikan.moe/v4/anime?q={title}&limit=1\"\n",
        "            resp = requests.get(url)\n",
        "            if resp.status_code == 200:\n",
        "                raw = resp.json()\n",
        "                if raw.get('data'):\n",
        "                    cand = raw['data'][0]\n",
        "                    # Year Logic\n",
        "                    y_j = cand.get('year') or (cand.get('aired', {}).get('prop', {}).get('from', {}).get('year'))\n",
        "                    match = True\n",
        "                    if year_trakt and y_j and abs(int(year_trakt) - int(y_j)) > 1: match = False\n",
        "\n",
        "                    if match:\n",
        "                        data = cand\n",
        "                        con.execute(\"INSERT INTO JIKAN_CACHE VALUES (?, ?, ?, ?)\", (title, data.get('mal_id'), json.dumps(data), datetime.now()))\n",
        "            time.sleep(1.05) # Rate limit\n",
        "        except Exception as e: print(f\"Err {title}: {e}\")\n",
        "\n",
        "        if data: results[title] = data\n",
        "\n",
        "    # 4. Format Output\n",
        "    final_map = {}\n",
        "    for t, data in results.items():\n",
        "        studio = \"Unknown\"\n",
        "        if data.get('studios'): studio = data['studios'][0]['name']\n",
        "        final_map[t] = {'MAL_SCORE': data.get('score'), 'STUDIO': studio, 'TOTAL_EPISODES': data.get('episodes')}\n",
        "\n",
        "    if should_close: con.close()\n",
        "    return final_map\n",
        "\n",
        "def enrich_genres_with_gemini(titles_list, batch_size=20, con=None):\n",
        "    if not titles_list: return {}\n",
        "    print(f\"\\nüß† Enriching {len(titles_list)} genres via Gemini...\")\n",
        "    should_close = False\n",
        "    if not con: con = duckdb.connect(DB_PATH); should_close = True\n",
        "\n",
        "    # 1. Bulk Load Cache\n",
        "    try: cached_rows = con.execute(\"SELECT TITLE, GENRES_JSON FROM GENRE_CACHE\").fetchall()\n",
        "    except: cached_rows = []\n",
        "    cache_map = {r[0]: json.loads(r[1]) for r in cached_rows}\n",
        "\n",
        "    # 2. Identify Delta\n",
        "    missing_titles = [t for t in titles_list if t not in cache_map]\n",
        "    print(f\"   - Cached: {len(titles_list)-len(missing_titles)} | New to Gen: {len(missing_titles)}\")\n",
        "\n",
        "    # 3. Generate Missing\n",
        "    try: model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "    except: return cache_map # Fallback\n",
        "\n",
        "    for i in range(0, len(missing_titles), batch_size):\n",
        "        batch = missing_titles[i:i+batch_size]\n",
        "        try:\n",
        "            prompt = f\"Classify titles into 2-3 genres. Titles: {json.dumps(batch)} Output STRICT JSON: {{'Title': ['Genre1']}}\"\n",
        "            response = model.generate_content(prompt)\n",
        "            text = response.text.replace('```json', '').replace('```', '')\n",
        "            new_data = json.loads(text)\n",
        "\n",
        "            # Update Cache & Map\n",
        "            for t, g_list in new_data.items():\n",
        "                cache_map[t] = g_list\n",
        "                con.execute(\"INSERT OR REPLACE INTO GENRE_CACHE VALUES (?, ?)\", (t, json.dumps(g_list)))\n",
        "            time.sleep(1)\n",
        "        except Exception as e: print(f\"Gen Error: {e}\")\n",
        "\n",
        "    if should_close: con.close()\n",
        "    return cache_map\n",
        "\n",
        "def calculate_viewing_stats(df):\n",
        "    stats_list = []\n",
        "    grouped = df.groupby('TRAKT_ID')\n",
        "    for trakt_id, group in grouped:\n",
        "        group = group.sort_values('watched_at')\n",
        "        stats_list.append({\n",
        "            'TRAKT_ID': trakt_id,\n",
        "            'EPISODES_WATCHED': len(group),\n",
        "            'FIRST_WATCHED_AT': group['watched_at'].min(),\n",
        "            'WATCH_DURATION_DAYS': (group['watched_at'].max() - group['watched_at'].min()).days,\n",
        "            'BINGE_INDICATOR': len(group) > 3 and group.set_index('watched_at').resample('D').size().max() > 3\n",
        "        })\n",
        "    return pd.DataFrame(stats_list)\n",
        "\n",
        "def load_trakt_files(pattern_list):\n",
        "    all_data = []\n",
        "    for pattern in pattern_list:\n",
        "        search_path = os.path.join(PROJECT_DIR, '**', pattern)\n",
        "        files = glob.glob(search_path, recursive=True)\n",
        "        print(f\"Searching '{pattern}'... Found {len(files)}\")\n",
        "        for f_path in files:\n",
        "            try:\n",
        "                with open(f_path, 'r') as f:\n",
        "                    content = json.load(f)\n",
        "                    if isinstance(content, list): all_data.extend(content)\n",
        "            except Exception as e: print(f\"Error loading {f_path}: {e}\")\n",
        "    return all_data\n",
        "\n",
        "def run_etl_pipeline():\n",
        "    print(f\"üöÄ Starting ETL in {PROJECT_DIR}...\")\n",
        "    conn = duckdb.connect(DB_PATH)\n",
        "    try:\n",
        "        # 1. Load Data\n",
        "        history_data = load_trakt_files(['watched-history-*.json', 'watched-movies.json'])\n",
        "        ratings_data = load_trakt_files(['ratings-shows.json', 'ratings-movies.json'])\n",
        "        if not history_data: return\n",
        "\n",
        "        rating_map = {}\n",
        "        for r in ratings_data:\n",
        "            item = r.get('show') or r.get('movie')\n",
        "            if item and r.get('rating'): rating_map[str(item['ids']['trakt'])] = r['rating']\n",
        "\n",
        "        # 2. Process History\n",
        "        raw_df = pd.DataFrame(history_data)\n",
        "        if 'movie' in raw_df.columns:\n",
        "             raw_df['title'] = raw_df.apply(lambda x: x['movie']['title'] if pd.notnull(x.get('movie')) else x['show']['title'], axis=1)\n",
        "             raw_df['year'] = raw_df.apply(lambda x: x['movie']['year'] if pd.notnull(x.get('movie')) else x['show']['year'], axis=1)\n",
        "             raw_df['id'] = raw_df.apply(lambda x: x['movie']['ids']['trakt'] if pd.notnull(x.get('movie')) else x['show']['ids']['trakt'], axis=1)\n",
        "             raw_df['type'] = raw_df.apply(lambda x: 'movie' if pd.notnull(x.get('movie')) else 'show', axis=1)\n",
        "\n",
        "        if 'watched_at' not in raw_df.columns and 'last_watched_at' in raw_df.columns:\n",
        "            raw_df['watched_at'] = raw_df['last_watched_at']\n",
        "\n",
        "        raw_prep = raw_df.rename(columns={'id': 'TRAKT_ID', 'title': 'TITLE', 'year': 'YEAR', 'type': 'MEDIA_TYPE'})\n",
        "        raw_prep['TRAKT_ID'] = raw_prep['TRAKT_ID'].astype(str)\n",
        "        raw_prep['watched_at'] = pd.to_datetime(raw_prep['watched_at'])\n",
        "        raw_prep['TITLE'] = raw_prep['TITLE'].apply(clean_title)\n",
        "        raw_prep['USER_RATING'] = raw_prep['TRAKT_ID'].apply(lambda x: rating_map.get(x, None))\n",
        "\n",
        "        stats_df = calculate_viewing_stats(raw_prep)\n",
        "        staging_base = raw_prep.sort_values('watched_at', ascending=False).drop_duplicates('TRAKT_ID')\n",
        "        staging_df = staging_base.merge(stats_df, on='TRAKT_ID', how='left')\n",
        "        staging_df = staging_df.rename(columns={'watched_at': 'LAST_WATCHED_AT'})\n",
        "\n",
        "        # 3. Optimized Enrichments\n",
        "        unique_items = staging_df[['TITLE', 'YEAR']].drop_duplicates().to_dict('records')\n",
        "        # Pass 'conn' to reuse connection and access cache tables\n",
        "        jikan_map = enrich_with_jikan(unique_items, con=conn)\n",
        "\n",
        "        unique_titles = [i['TITLE'] for i in unique_items]\n",
        "        genre_map = enrich_genres_with_gemini(unique_titles, con=conn)\n",
        "\n",
        "        staging_df['GENRES'] = staging_df['TITLE'].apply(lambda t: json.dumps(genre_map.get(t, [])))\n",
        "        staging_df['MAL_SCORE'] = staging_df['TITLE'].apply(lambda t: jikan_map.get(t, {}).get('MAL_SCORE'))\n",
        "        staging_df['STUDIO'] = staging_df['TITLE'].apply(lambda t: jikan_map.get(t, {}).get('STUDIO'))\n",
        "        staging_df['TOTAL_EPISODES'] = staging_df['TITLE'].apply(lambda t: jikan_map.get(t, {}).get('TOTAL_EPISODES'))\n",
        "\n",
        "        # 4. Upsert (SCD2)\n",
        "        cols = ['TRAKT_ID', 'TITLE', 'USER_RATING', 'MEDIA_TYPE', 'LAST_WATCHED_AT', 'EPISODES_WATCHED', 'FIRST_WATCHED_AT', 'WATCH_DURATION_DAYS', 'BINGE_INDICATOR', 'GENRES', 'MAL_SCORE', 'STUDIO', 'TOTAL_EPISODES', 'YEAR']\n",
        "        for c in cols:\n",
        "             if c not in staging_df.columns: staging_df[c] = None\n",
        "\n",
        "        current_db_df = conn.query(\"SELECT * FROM USER_WATCH_HISTORY WHERE IS_CURRENT = TRUE\").to_df()\n",
        "        if not current_db_df.empty: current_db_df = current_db_df.astype({'TRAKT_ID': str})\n",
        "\n",
        "        merged = staging_df.merge(current_db_df, on='TRAKT_ID', how='left', suffixes=('', '_OLD'))\n",
        "        new_records = merged[merged['TITLE_OLD'].isna()].copy()\n",
        "        changed_records = merged[(merged['TITLE_OLD'].notna()) & ((merged['USER_RATING'] != merged['USER_RATING_OLD']) | (merged['EPISODES_WATCHED'] != merged['EPISODES_WATCHED']))].copy()\n",
        "\n",
        "        now = datetime.now()\n",
        "        if not changed_records.empty:\n",
        "            ids = tuple(changed_records['TRAKT_ID'].tolist())\n",
        "            if len(ids) == 1: ids = f\"('{ids[0]}')\"\n",
        "            conn.execute(f\"UPDATE USER_WATCH_HISTORY SET VALID_TO=?, IS_CURRENT=FALSE WHERE TRAKT_ID IN {ids} AND IS_CURRENT=TRUE\", [now])\n",
        "\n",
        "        to_insert = pd.concat([new_records, changed_records])\n",
        "        if not to_insert.empty:\n",
        "            final_df = to_insert[cols].copy()\n",
        "            final_df['VALID_FROM'] = now; final_df['VALID_TO'] = None; final_df['IS_CURRENT'] = True; final_df['INGESTION_TIMESTAMP'] = now\n",
        "            c_order = ['TRAKT_ID', 'TITLE', 'YEAR', 'USER_RATING', 'MEDIA_TYPE', 'GENRES', 'EPISODES_WATCHED', 'TOTAL_EPISODES', 'FIRST_WATCHED_AT', 'LAST_WATCHED_AT', 'WATCH_DURATION_DAYS', 'BINGE_INDICATOR', 'STUDIO', 'MAL_SCORE', 'VALID_FROM', 'VALID_TO', 'IS_CURRENT', 'INGESTION_TIMESTAMP']\n",
        "            final_df = final_df.reindex(columns=c_order)\n",
        "            conn.execute(\"INSERT INTO USER_WATCH_HISTORY SELECT * FROM final_df\")\n",
        "            print(f\"Inserted {len(final_df)} new/updated records.\")\n",
        "        else: print(\"No new changes detected.\")\n",
        "    except Exception as e: print(f\"ETL Error: {e}\")\n",
        "    finally: conn.close()\n",
        "\n",
        "run_etl_pipeline()"
      ],
      "id": "9rr4XX5sjxoo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPmGMMbejxoo"
      },
      "outputs": [],
      "source": [
        "# Step 5: DATA VERIFICATION\n",
        "print(\"--- DATABASE AUDIT ---\")\n",
        "con = duckdb.connect(DB_PATH)\n",
        "try:\n",
        "    count = con.execute(\"SELECT COUNT(*) FROM USER_WATCH_HISTORY WHERE IS_CURRENT=TRUE\").fetchone()[0]\n",
        "    print(f\"üé• Active Watch History Records: {count}\")\n",
        "    j_count = con.execute(\"SELECT COUNT(*) FROM JIKAN_CACHE\").fetchone()[0]\n",
        "    g_count = con.execute(\"SELECT COUNT(*) FROM GENRE_CACHE\").fetchone()[0]\n",
        "    print(f\"üíæ Cache Stats: Jikan={j_count} | Genres={g_count}\")\n",
        "\n",
        "    if count > 0:\n",
        "        print(\"\\nSnapshot (Top 5 Recent):\")\n",
        "        df = con.query(\"SELECT TITLE, YEAR, USER_RATING, EPISODES_WATCHED, LAST_WATCHED_AT FROM USER_WATCH_HISTORY WHERE IS_CURRENT=TRUE ORDER BY LAST_WATCHED_AT DESC LIMIT 5\").to_df()\n",
        "        display(df)\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Table is empty! ETL failed.\")\n",
        "except Exception as e: print(f\"Audit Error: {e}\")\n",
        "finally: con.close()"
      ],
      "id": "BPmGMMbejxoo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bk52bn4wjxop"
      },
      "outputs": [],
      "source": [
        "# Step 6: AI Engine\n",
        "def get_ai_recommendations():\n",
        "    con = duckdb.connect(DB_PATH)\n",
        "    try: context_df = con.query(\"SELECT TITLE, USER_RATING, BINGE_INDICATOR, STUDIO FROM USER_WATCH_HISTORY WHERE IS_CURRENT = TRUE AND USER_RATING >= 8 ORDER BY BINGE_INDICATOR DESC, LAST_WATCHED_AT DESC LIMIT 50\").to_df()\n",
        "    except: return []\n",
        "    finally: con.close()\n",
        "\n",
        "    favorites = context_df['TITLE'].tolist()\n",
        "    studios = context_df['STUDIO'].dropna().unique().tolist()[:3]\n",
        "\n",
        "    selected_services = [chk.description for chk in service_checkboxes if chk.value]\n",
        "    selected_genres = list(genre_selector.value)\n",
        "    current_mood = mood_selector.value\n",
        "    mode = discovery_mode.value\n",
        "    length_pref = time_commitment.value\n",
        "\n",
        "    if not selected_services or not selected_genres:\n",
        "        print(\"‚ö†Ô∏è Configure preferences above.\")\n",
        "        return []\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Role: Anime Recommender AI.\n",
        "    Output STRICT JSON: [{{'title': '...', 'reasoning': '...', 'service': '...', 'ai_confidence': 0.95}}]\n",
        "\n",
        "    User Profile:\n",
        "    - Favorites: {', '.join(favorites)}\n",
        "    - Studios: {', '.join(studios)}\n",
        "    - Services: {', '.join(selected_services)}\n",
        "    - Genres: {', '.join(selected_genres)}\n",
        "\n",
        "    Context:\n",
        "    - Mood: {current_mood}\n",
        "    - Length: {length_pref}\n",
        "    - Mode: {mode}\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Asking Gemini...\")\n",
        "    try:\n",
        "        model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "        response = model.generate_content(prompt)\n",
        "        text = response.text.replace('```json', '').replace('```', '')\n",
        "        return json.loads(text)\n",
        "    except Exception as e:\n",
        "        print(f\"AI Error: {e}\")\n",
        "        return []"
      ],
      "id": "Bk52bn4wjxop"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oylLnZrjxop"
      },
      "outputs": [],
      "source": [
        "# Step 7: UI\n",
        "def save_to_watchlist(title, service, notes, priority, confidence=None):\n",
        "    con = duckdb.connect(DB_PATH)\n",
        "    try:\n",
        "        id = con.execute(\"SELECT nextval('seq_plan_id')\").fetchone()[0]\n",
        "        con.execute(\"INSERT INTO PLAN_TO_WATCH (ID, TITLE, RECOMMENDED_BY_AI_AT, STREAMING_SERVICE, NOTES, PRIORITY, CONFIDENCE_SCORE) VALUES (?, ?, ?, ?, ?, ?, ?)\", (id, title, datetime.now(), service, notes, priority, confidence))\n",
        "        print(f\"\\n[Saved] {title} ({priority} Priority)\")\n",
        "    except Exception as e: print(f\"Error saving: {e}\")\n",
        "    finally: con.close()\n",
        "\n",
        "def display_recommendations():\n",
        "    recs = get_ai_recommendations()\n",
        "    if not recs: return\n",
        "    rows = []\n",
        "    for i, item in enumerate(recs):\n",
        "        t, s, r, c = item.get('title'), item.get('service'), item.get('reasoning'), item.get('ai_confidence', 0.85)\n",
        "        l1 = widgets.HTML(f\"<b>{i+1}. {t}</b> <span style='color:gray'>({s})</span>\")\n",
        "        l2 = widgets.Label(f\"{r}\")\n",
        "        p_dropdown = widgets.Dropdown(options=['High', 'Medium', 'Low'], value='Medium', description='Priority:', layout=widgets.Layout(width='200px'))\n",
        "        btn = widgets.Button(description='Save', icon='plus', button_style='success')\n",
        "        def on_click(b, t=t, s=s, r=r, c=c, p=p_dropdown):\n",
        "            save_to_watchlist(t, s, r, p.value, c)\n",
        "            b.icon = 'check'; b.description = 'Saved'; b.disabled = True\n",
        "        btn.on_click(on_click)\n",
        "        rows.append(widgets.VBox([l1, l2, widgets.HBox([p_dropdown, btn]), widgets.HTML(\"<hr>\")]))\n",
        "    display(widgets.VBox(rows))\n",
        "\n",
        "display_recommendations()"
      ],
      "id": "0oylLnZrjxop"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}